
---
title: "PADP8120_Homework5"
author: "Prisca Lee"
date: "December 2, 2015"
output:
  html_document:
    highlight: pygments
    theme: cerulean
  word_document: default
widgets: mathjax
---


# Homework 5

Guidelines: Homeworks should be clear and legible, with answers clearly indicated and work shown. Homeworks will be given a minus, check, or check plus owing to completion and correctness. You are welcome to work with others but please submit your own work. Your homework must be produced in an R Markdown (.rmd) file submitted via github. If you are having trouble accomplishing this, please refer to the [guide](http://spia.uga.edu/faculty_pages/tyler.scott/teaching/PADP8120_Fall2015/Homeworks/submitting_homework.shtml). 


This homework adapts materials from the work of Michael Lynch (http://spia.uga.edu/faculty_pages/mlynch/) and Matthew Salganik (http://www.princeton.edu/~mjs3/)

## Topics

Topics covered in this homework include:

- Matrix regression 
- Interactions and categorical variables
- Transformations
- Maximum Likelihood

## Problems

### Problem 1. 

Just as you did for Homework 4, write a function that emulates the `lm` function in R for a simple (bivariate) regression. *However, this time your function needs to make use of the matrix regression approach we learned in Week 12.* Like the `lm` function, your function should be able to estimate and report to the screen $\beta_k$ coefficients, standard errors for these coefficients, and corresponding t-values and p-values. It should also report the residual standard error. Be sure to show your code. Compare your results to the results of the `lm` function on some data of your choosing to verify that things are working correctly. 

```{r}
sim.x1 <- rnorm(100)
sim.x2 <- rnorm(100)
sim.y <- matrix(rnorm(100),ncol=1)
x.vars <- as.matrix(data.frame(intercept <- 1,sim.x1,sim.x2))

matrix.lm <- function(outcome.matrix,design.matrix)
{
betas <- solve(t(design.matrix) %*% design.matrix) %*% t(design.matrix) %*% outcome.matrix
betas <- round(betas,3)
dSigmaSq <- sum((outcome.matrix - design.matrix%*%betas)^2)/(nrow(design.matrix)-ncol(design.matrix))
VarCovar <- dSigmaSq*chol2inv(chol(t(design.matrix)%*%design.matrix)) 
vStdErr <- round(sqrt(diag(VarCovar)),3)
df <- nrow(outcome.matrix) - length(betas)
t.obs <- round(betas/vStdErr,3)
p.vals <- round(2 * (1-pt(abs(t.obs),df=df)),3)
return(data.frame(coefs = betas,SE = vStdErr,
                  t.obs = t.obs,p.vals=p.vals))}

matrix.lm(sim.y,x.vars)

summary(lm(sim.y~sim.x1+sim.x2))
```

### Problem 2.

#### Occupational prestige 

Let's try to understand the relationship between typical education, income, job type, and occupation prestigue using the data from Duncan.  You can read the documentation [here](http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-2E/datasets/Duncan.pdf)

Here's some code to read in and clean the data.  And for the purposes of this assignment we are going to exclude professionals.  In other words, we are only concerned about white collar and blue collar occupations.  Again, notice that the unit here is occupations.

```{r message=FALSE,warnings=FALSE}
library(dplyr)
occup <- read.table("input/Duncan.txt", header=TRUE)
occup$state <- rownames(occup)
rownames(occup) <- NULL
occup <- filter(occup, type %in% c("wc", "bc"))
head(occup)
```

(@) Run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950).

```{r}
mod1 <- lm(prestige~education, occup)
summary(mod1)
```

The prestige of an occupation based on the level of education of people in that occupation is significant at the 0.001 level.

(@) Make a plot showing the data and the model that you fit.

```{r}
plot(prestige~education,occup)
abline(reg = mod1)
```

(@) Now run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950) and the occupation type (blue collar/white collar).

```{r}
mod2 <- lm(prestige~education+type, occup)
summary(mod2)
```

When a multiple regression model is run to predict the prestige of an occupation based on the level of education of people in that occupation and the occupation type it is significant at the 0.01 level.

(@) Make a plot showing the data and the model that you fit.

```{r}
plot (prestige~education,occup)
abline(reg = mod2)
```

(@) Now run a regression model to predict occupational prestige based on the level of education and occupation type where the relationship between education and occupational prestige is allowed to vary by occupation type.

```{r}
mod3 <- lm(prestige~education*type, occup)
summary(mod3)
```

When predicting occupational prestige based on the level of education and occupation type where the relationship between education and occupational prestige is allowed to vary by occupation, it is significant at the 0.01 level. 

(@) Calculate predicted levels of prestige for white collar and blue collar jobs at various levels of income and report these predicted levels in a graph (no table needed). What have you learned about prestige thanks to the interactive variable?

```{r}
predicted.vals <- predict(mod3, newdata = data.frame (education=rep(seq(0,100,10),2), type=rep(c('wc','bc'),each=11)))
plot(x=rep(seq(0,100,10),2),y=predicted.vals,col=rep(c('grey80','blue'),each=11),pch=19,xlab='Education',ylab='Predicte Prestige')
```

(@) How would you summarize the conclusions from three models above?

The first model regressing prestige as a function of education fits the data best at a significance level of 0.001. The second and third multiple regressions are signficant at the 0.01 level. Models 2 and 3 indicates that the level of education in a given profession positively correlates with occupational prestige for both while and blue collar jobs. However, the slope for white collar jobs is flatter, indicating that it is less dependent on the level of education than blue collar jobs. 

(@) Now run a the following regression model: `lm(prestige ∼ income + education + income ∗ education)` and substantively describe the effects of the independent variables on the dependent variable. In other words, describe the relationships implied by the interactive terms. Does this interaction make sense to you? Why or why not? No table needed.

```{r}
mod4 <- lm(prestige ~income+ education + income * education,occup)
summary(mod4)
```

Model four is the regression of occupational prestige on income, education, and the interactive terms of income and education. As education increases, the marginal impact of income on pretige dereases. Although to a lesser extent, the same is true for education. Consequently, this means that a prestigious job could have high income, high education, or a combination of both. 

(@) Use calculus to identify the predicted impact of a one unit change in income on occupational prestige. Assess whether this impact is statistically distinct from zero. Remember that the variance for an estimated marginal effect $\frac{\partial \hat{y}}{\partial x} = \hat{\beta}_x + \hat{\beta}_{xz}*z$ where x and z are interacted independent variables, can be calculated by:

$$ V(\frac{\partial \hat{y}}{\partial x}) = V(\hat{\beta}_x) + z^2 V(\hat{\beta}_{xz}) + 2z * Cov(\hat{\beta}_x,\hat{\beta}_{xz}) $$

```{r}
educ.sim <- seq(1:100)
bx <- mod4$coef[2]
bxz <- mod4$coef[4]
inc.slopes <- bx + bxz * educ.sim
var.bx <- vcov(mod4)[2,2]
var.bxz <- vcov(mod4)[4,4]
cov.bx.bxz <- vcov(mod4)[4,2]
var.dy.dx <- var.bx + educ.sim^2 * var.bxz + 2*educ.sim*cov.bx.bxz
p.vals <- 2 * (1-pt(abs(inc.slopes/sqrt(var.dy.dx)),df=nrow(occup)-length(mod4)))
plot(p.vals~educ.sim,ylim=c(0,1),ylab='p-value',xlab='Education')
abline(h=0.05,col='red',lty=2)
```

The predicted impact of a one unit change on income in occupational prestige is significant at the 0.05 level when education is equal to or less than about 65.

(@) ￼Because the marginal effect of x depends on values of z, you will need to assess whether the marginal effect is significant across a range of values of z.

```{r}
upper <- inc.slopes + 1.96*sqrt(var.dy.dx)
lower <- inc.slopes - 1.96*sqrt(var.dy.dx)
plot(educ.sim, inc.slopes , type = "l", lty = 1, xlab = "Level of Education", ylab = "Marginal Effect of Income",ylim=c(-1,1))
points(educ.sim, upper, type = "l", lty = 2)
points(educ.sim, lower, type = "l", lty = 2)
points(educ.sim, rep(0, length(educ.sim)), type = "l", col = "gray")
```

Once again, the 95% confidnce interval for the marginal effect of income on prestige does not include 0 for levels of education below 65.

### Problem 3.

#### LA Housing Prices

Load the LA housing prices dataset:

```{r}
la.dat = read.csv('Input/LA.csv')
```

(@) Fit the best model you can to predict housing prices in LA on the basis of theory (i.e., what should matter for house prices?) and model fit (i.e., DO NOT use stepwise regression, but feel free to add/subtract/transform variables as you feel are necessary).

On the basis of theory, housing prices will vary depending on the number of bedrooms, the number of bahtrooms, square footage, the number of garage spots, type, and whether or not it has a pool. 

First, the data needs to be cleaned.

```{r}
la.dat$garage = as.numeric(ifelse(is.na(la.dat$garage),0,ifelse(la.dat$garage=='',0,ifelse(as.character(la.dat$garage)=='4+',4,as.character(la.dat$garage)))))

la.dat$pool = ifelse(la.dat$pool=='Y',1,0)
```

Now, different models can be created with the variables to find the best fit. 

The first model regresses the log of price on the log of squarefeet, the number of bedrooms, the number of bathrooms, whether or not the house has a pool, and the number of garage bays with an interaction term between squarefeet and home type beccuase of the anticipated marginal impact of additional squarefeet on price for different classes of homes.

```{r}
la.mod1 <- lm(log(price)~log(sqft)*type+bed+bath+pool+garage,la.dat)
summary(la.mod1)
```

The second model removes the interaction term.

```{r}
la.mod2 <- lm(log(price)~log(sqft)+type+bed+bath+pool+garage,la.dat)
summary(la.mod2)
```

The third model removes home type as a variable. 

```{r}
la.mod3 <- lm(log(price)~log(sqft)+bed+bath+pool+garage,la.dat)
summary(la.mod3)
library(stargazer)
library(knitr)
stargazer(la.mod1,la.mod2,la.mod3,type='text',omit.stat=c("f", "rsq"),column.labels = c('M1',"M2","M3"),model.names = F,model.numbers = F)
BIC(la.mod1,la.mod2,la.mod3)
```

When comparing all three models, it appears that home type does not make a large difference. Becuase model 3 has the fewest number of parameters and only a marginally higher BIC score, it is the best model of the three. 

(@) Demonstrate the goodness-of-fit of your model (i.e, show that key assumptions appear to be met and that the model would seem to be a viable basis for inference). 

Various functions can test if the model appears to meet key assumptions.
```{r}
hist(la.mod3$residuals,breaks=100)
plot(la.mod3,2)
plot(la.mod3,1)
plot(la.mod3,5)
```

The residuals appear to be normally distributed, and the quantile-quantile plots indicate a common distribution. The residuals also appear to be randomly distribute around zero, and the variance is constant. Finally, there are no outliers that have high influence.

(@) Interpret your substantive findings.

```{r}
summary(la.mod3)
```

According to this model, additional bedrooms and garage bays lead to a decrease in price while pools, bathrooms, and additional squarefootage predicts an increase in price. 

(@) Discuss any potential shortcomings of this model and key future directions that you might take if you sought to better understand LA housing prices. 

A major shortcoming of the data is a major factor in housing prices was not included- neighborhoods, or surrounding house prices. Houses located near each other often have similar values, so it is important to include this.

### Problem 4.

(@) Again, using the LA housing price data, fit a model that estimates sqrt(price) solely as a function of sqrt(square footage) using maximum likelihood estimation (MLE) (hint: you'll need to use the `mle` function from the `stats4` package). Recall that in a linear regression, we assume that the residuals are normally distributed, so for MLE in this case we want a likelihood function that fits a normal distribution to the residuals:

```{r}
LL <- function(beta0, beta1, mu, sigma) {
    R = sqrt(la.dat$price) - sqrt(la.dat$sqft) * beta1 - beta0
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    -sum(R)
}
library(stats4)
ml.est = mle(LL, start = list(beta0 = 10, beta1 = 10, sigma=1),fixed = list(mu = 0), nobs = length(la.dat$price))
summary(ml.est)

```

(@) Try several different starting parameter values: How consistent are your results? Do your results typically match up with results from a simple linear regression? What do you think accounts for your results?

```{r}
summary(lm(sqrt(price)~sqrt(sqft),data = la.dat))
```

The resuls are close to the original. If starting values use the square root, a one unit increase in the square root of a square foot is predicted to increase the square root of the house price by 37.78. The same predicted value from the OLS model is 37.91.

(@) Perform the same analysis with but with the addition of a variable for number of bathrooms (i.e., $price ~ size + bathrooms$).

```{r}
LL2 <- function(beta0, beta1, beta2, mu, sigma) {
    R = sqrt(la.dat$price) - 
      la.dat$bath * beta2 - sqrt(la.dat$sqft) * beta1 - beta0
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    -sum(R)
}
ml.est2 = mle(LL2, start = list(beta0 = 10, beta1 = 10, beta2 = 10,sigma=1), fixed = list(mu = 0), nobs = length(la.dat$price))
summary(ml.est2)
```

(@) In NO MORE than 4-6 sentences, explain how maximum likelihood estimation works in the context of this problem (i.e, how might you briefly describe your modeling approach within the context of a journal article methods section?)

Miximum likelihood estimation gives the parameter with the highest explanatory power for given data. In the context of this problem, it means that we identify estimates for different betas that maximize the likelihood function.

### Report your process

You're encouraged to reflect on what was hard/easy, problems you solved, helpful tutorials you read, etc. Give credit to your sources, whether it's a blog post, a fellow student, an online tutorial, etc.

Similar to the previous assignment, writing a fucntion that emulates the lm function was difficult. Referencing HW4 did make it easier this time around. Additionally, cleaning the dataset was more challenging because of the missing data and how it was coded orginially. Once again, previous homework assignments and labs were good references. 

### Rubric

Minus: Didn't tackle at least 3 tasks. Or didn't make companion graphs. Didn't interpret anything but left it all to the "reader". Or more than one technical problem that is relatively easy to fix. It's hard to find the report in our repo.

Check: Completed, but not fully accurate and/or readable. Requires a bit of detective work on my part to see what you did

Check plus: Hits all the elements. No obvious mistakes. Pleasant to read. No heroic detective work required. Solid.



#### The command below is helpful for debugging, please don't change it

```{r echo=FALSE}
sessionInfo()
```









