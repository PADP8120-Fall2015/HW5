
---
title: "PADP8120_Homework5"
author: "Victoria Coxon Fall 2015"
date: "![Creative Commons Attribution License](images/cc-by.png)"
output:
  html_document:
    highlight: pygments
    theme: cerulean
widgets     : [mathjax]            # {mathjax, quiz, bootstrap}
---


# Homework 5

Guidelines: Homeworks should be clear and legible, with answers clearly indicated and work shown. Homeworks will be given a minus, check, or check plus owing to completion and correctness. You are welcome to work with others but please submit your own work. Your homework must be produced in an R Markdown (.rmd) file submitted via github. If you are having trouble accomplishing this, please refer to the [guide](http://spia.uga.edu/faculty_pages/tyler.scott/teaching/PADP8120_Fall2015/Homeworks/submitting_homework.shtml). 


This homework adapts materials from the work of Michael Lynch (http://spia.uga.edu/faculty_pages/mlynch/) and Matthew Salganik (http://www.princeton.edu/~mjs3/)

## Topics

Topics covered in this homework include:

- Matrix regression 
- Interactions and categorical variables
- Transformations
- Maximum Likelihood

## Problems

### Problem 1. 

Just as you did for Homework 4, write a function that emulates the `lm` function in R for a simple (bivariate) regression. *However, this time your function needs to make use of the matrix regression approach we learned in Week 12.* Like the `lm` function, your function should be able to estimate and report to the screen 
- $\beta_k$ coefficients, 
- standard errors for these coefficients, and
- corresponding t-values and p-values. 
- It should also report the residual standard error. 
Be sure to show your code. Compare your results to the results of the `lm` function on some data of your choosing to verify that things are working correctly.

###### Formulas

1. The model is: __$\mathbf{Y} = \mathbf{X} \mathbf{\beta} + \mathbf{\epsilon}$__

2. We want to find __$\mathbf{b}$__ that minimizes the residual sum of squares.

3. This is given by: __$\mathbf{b} = \mathbf{(X'X)^{-1}X'y}$__
```{r}
x1 = rnorm(100)
x2 = rnorm(100)
y = matrix(rnorm(100),ncol=1)
#design matrix
x.vars = as.matrix(data.frame(intercept = 1,x1,x2))

#coefficient estimates
matrix.lm = function(outcome.matrix,design.matrix)
  {
# coefficient estimates
  betas = solve(t(x.vars) %*% x.vars) %*% t(x.vars) %*% outcome.matrix
betas = round(betas,5)
#estimate of sigma-squared
dSigmaSq <- sum((outcome.matrix - x.vars%*%betas)^2)/(nrow(x.vars)-ncol(x.vars))
#variance covariance matrix
VarCovar <- dSigmaSq*chol2inv(chol(t(x.vars)%*%x.vars))
#coeff. est. standard errors  
vStdErr <- sqrt(diag(VarCovar))
#
df = nrow(outcome.matrix) - length(betas)
#t-score observed
t.obs = round(betas/vStdErr,5)
#p values
p.vals = round(2*(1 - pt(abs(t.obs),df = df)),5)
#print it out in a table
return(data.frame(coefs = betas,SE = vStdErr,t.obs = t.obs,p.vals = p.vals))}

matrix.lm(y,x.vars)

summary(lm(y~x1+x2))
```
### Problem 2.

#### Occupational prestige 

Let's try to understand the relationship between typical education, income, job type, and occupation prestige using the data from Duncan.  You can read the documentation [here](http://socserv.socsci.mcmaster.ca/jfox/Books/Applied-Regression-2E/datasets/Duncan.pdf)

Here's some code to read in and clean the data.  And for the purposes of this assignment we are going to exclude professionals.  In other words, we are only concerned about white collar and blue collar occupations.  Again, notice that the unit here is occupations.

```{r message=FALSE,warnings=FALSE}
library(dplyr)
occup <- read.table("https://raw.githubusercontent.com/vcoxon/Labs/master/input/Duncan.txt", header=TRUE)
occup$state <- rownames(occup)
rownames(occup) <- NULL
occup <- filter(occup, type %in% c("wc", "bc"))
head(occup)
```

(@) Run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950).
```{r}
summary(model1 <- lm(prestige ~ education,occup))
```

(@) Make a plot showing the data and the model that you fit.
```{r}
library(ggplot2)
library(ggthemes)

plot(prestige ~ education, data = occup)
abline(reg = model1)
```

(@) Now run a regression model to predict the prestige of an occupation based on the level of education of people in that occupation (measured by the percentage of people in the field who have graduated from high school in 1950) and the occupation type (blue collar/white collar).
```{r}
summary(model2 <- lm(prestige ~ education + type, data = occup))
```
(@) Make a plot showing the data and the model that you fit.
```{r}
plot(prestige ~ education, occup)
#model1 line
abline(a = model2$coefficients[1], b = model2$coefficients[2], col = 'deepskyblue', lwd = 3)
#model2 line
abline(a = model2$coefficients[1] + model2$coefficients[3], b = model2$coefficients[2], col = 'deepskyblue4', lwd = 3)
```
(@) Now run a regression model to predict occupational prestige based on the level of education and occupation type where the relationship between education and occupational prestige is allowed to vary by occupation type.
```{r}
summary(model3 <- lm(prestige ~ education * type, data = occup))
```

(@) Calculate predicted levels of prestige for white collar and blue collar jobs at various levels of income and report these predicted levels in a graph (no table needed). What have you learned about prestige thanks to the interactive variable?
```{r}
pred.vals = predict(model3,newdata = data.frame(education = rep(seq(0,100,10),2),type = rep(c('wc','bc'),each = 11)))

plot(x = rep(seq(0,100,10),2),y = pred.vals, col = rep(c('deepskyblue4','deepskyblue'), each = 11), pch = 19,xlab = 'Education', ylab = 'Predicted Prestige') 
1.0112 - 0.6323
```
(@) How would you summarize the conclusions from three models above?
*As demonstrated by the graphs, there is a positive relationship between a profession's level of education and occupational prestige for people holding both white collar and blue collar jobs.*

*Interstingly, it appears that additional education may not bring equivalent gains in occcupational prestige for white collar workers compared to blue collar workers. Model 3 predicts that for each one unit increase in degrees held by blue collar workers, occupational prestige should increase by approximately 1.01 units; for white collar workers, occupational prestige is predicted to increase by 0.3789 or approximately 0.38 (Beta1 - Beta3; 1.0112 - 0.6323).*

*It also appears that the higher level of education a profession demands, the higher the level of occupational prestige. This also seems to confirm that it is the **level of education** that is associated with prestige and not the type of job (blue collar or white collar). However, Models 2 and 3, with different intercepts, hint that the base level of education required in both white collar and blue collar jobs may be different.*

*Model 1, where prestige was a sole function of education, appeared to fit the data better than both Models 2 (prestige = Beta0 + education + type) and 3 (prestige = Beta0 + education(type). **For confirmation of this statement, the models need to be checked with ANOVA.** 
```{r}
library(car)
anova(model1,model2,model3)
```
*An ANOVA analysis of the models indicates that there is not a significant difference between the models. Maybe we should just go with the simpler model...*  


```{r}
tapply(occup$education, occup$type, mean)
boxplot(prestige ~ type, data = occup,names = c("BC","Professional" ,"WC"), col = c("cadetblue1", "gainsboro","antiquewhite"), xlab = "Type of Occupation")
```

*The graph confirms my hunch that the base level (and by proxy, the mean level) of education is different for white collar and blue colllar professions.*

(@) Now run the following regression model: `lm(prestige ∼ income + education + income ∗ education)` and substantively describe the effects of the independent variables on the dependent variable. In other words, describe the relationships implied by the interactive terms. Does this interaction make sense to you? Why or why not? No table needed.
```{r}
summary(model4 <- lm(prestige ∼ income + education + income ∗ education, occup))
```

*Model 4 regresses prestige on income, education, and the interaction of income given an education level.*

*The predicted impact of a one unit change in income on occupational prestige is approximately 0.737786 + 0.256704(-0.003559). According to our model, the relationship between income and occupational prestige is conditional on education; as an individual's educational level rises, income plays a diminishing role on predicted occupational prestige.*   

*Conversely, the predicted impact of a one unit increase in education on occupatinal prestige is approximately 0.256704 + 0.737786(-0.003559). According to our model, the relationship between education and occupational prestige is conditional on income; as an individual's income level rises, education plays a diminishing role on predicted occupational prestige.*  

*It would appear that education and income are proxies or substitutes for one another when it comes to predicting occupational prestige. Considering this, it could be useful to reevaluate the necessity of including the interaction term; income(education).*
```{r}
model5 = lm(prestige ~ income + education, occup)
anova(model5,model4)
```
*We fail to reject the null that there is a difference between models 4 and 5 (0.5471 > 0.05). Therefore, in the spirit of parsimony, model 5 is the better model because it seems to reduce the "noise" from the interaction variable (income * education).*

(@) Use calculus to identify the predicted impact of a one unit change in income on occupational prestige. Assess whether this impact is statistically distinct from zero. 

Remember that the variance for an **estimated marginal effect** 
$\frac{\partial \hat{y}}{\partial x} = \hat{\beta}x + \hat{\beta}{xz}*z$ 
where x and z are interacted independent variables, can be calculated by:

$$ V(\frac{\partial \hat{y}}{\partial x} = V(\hat{\beta}_x) + z^2 V(\hat{\beta}_{xz} + 2z * Cov(\hat{\beta}_x,\hat{\beta}_{xz}) $$
```{r}
model4
##compute marginal effect for different income levels (income = 1 to 100)
education.sim = seq(1:100)
Beta_xhat = model4$coefficients[2]
Beta_xzhat =model4$coefficients[4]
inc.slopes = Beta_xhat + Beta_xzhat * education.sim

#compute variance for interaction term
var.Beta_xhat = vcov(model4)[2,2]
var.Beta_xzhat = vcov(model4)[4,4]
cov.Beta_xhat.Beta_xzhat = vcov(model4)[2,4]
#now for the formula...
var.dyhat.dx = var.Beta_xhat + education.sim^2*var.Beta_xzhat + 2*education.sim*cov.Beta_xhat.Beta_xzhat

#p-values for 2 tails...
p.vals = 2 * (1 - pt(abs(inc.slopes/sqrt(var.dyhat.dx)),df = nrow(occup) - length(model4)))

plot(p.vals ~ education.sim, ylim = c(0,1), ylab = 'p-value', xlab = 'Income')
abline(h = 0.05, col = 'firebrick1', lty = 2)
```

*This plot demonstrates that the marginal effect/impact of income is significant at the $\alpha = 0.05$ level when education levels are below 60. After education reaches approximately 63-65, income has an increasingly insignficant effect/impact.*

(@) ￼Because the marginal effect of x depends on values of z, you will need to assess whether the marginal effect is significant across a range of values of z.
```{r}
#Maximum Likelihood Estimation?
#x=income
#z=education

upper.bound <- inc.slopes + 1.96*sqrt(var.dyhat.dx)
lower.bound <- inc.slopes - 1.96*sqrt(var.dyhat.dx)

plot(education.sim, inc.slopes, type ="l", lty = 1, xlab = "Education Level", ylab = "Income Marginal Effect", ylim = c(-1,1))

points(education.sim, upper.bound, type = "l", lty = 2)
points(education.sim, lower.bound, type = "l", lty = 2)
points(education.sim, rep(0,length(education.sim)), type = "l", col = "gray48")
```

*The 95% confidence interval for the marginal effect of income does not include 0 for levels of education below 60. This plot supports our previous finding in number 9.*

### Problem 3.

#### LA Housing Prices

Load the LA housing prices dataset:

```{r}
la.house.data = read.csv('https://raw.githubusercontent.com/vcoxon/HW5/master/input/LA.csv')

#Find all the missing values for each variable.
summary(la.house.data)
```

(@) Fit the best model you can to predict housing prices in LA on the basis of theory (i.e., what should matter for house prices?) and model fit (i.e., DO NOT use stepwise regression, but feel free to add/subtract/transform variables as you feel are necessary).

*My theoretical expectation is that price of the house is contingent on the quality of the local public school district. That particular data is not included in this data set, but it is something to think about going forward.* 

*Additional variables supported by theory that impact the price of housing in LA are the type of housing (SFR v. Townhome/Condo), bedrooms, bathrooms, square footage, a pool, and the number of garage bays.* 

*There are variables that need to be reformatted so we can properly analyze them.*
```{r}
la.house.data = read.csv('https://raw.githubusercontent.com/vcoxon/HW5/master/input/LA.csv')
#I know this looks stupid to re-import the data, but I was having problems knitting and this website offered a solution: http://stackoverflow.com/questions/32811527/error-in-evalexpr-envir-enclos-during-knit-in-r-markdown

#Garage: need to recode NA to 0 
#Step 1
la.house.data$garage <- ifelse(la.house.data$garage=="",0,as.character(la.house.data$garage))
#Step 2
la.house.data$garage <- ifelse(is.na(la.house.data$garage),0,as.character(la.house.data$garage))
#Step 3: Need to convert 4+(character) to 4(number)
la.house.data$garage <- ifelse(la.house.data$garage=="4+",4,la.house.data$garage)
#When I was doing the homework I checked to see that the command was successful via  'la.house.data$garage'. I won't do that here...
sum(la.house.data$garage=="4")
sum(la.house.data$garage=="3")
sum(la.house.data$garage=="2")
sum(la.house.data$garage=="1")
sum(la.house.data$garage=="0")
```

*However, I still have a problem. I converted "4+" into "4", since I would like to use garage variable as not dummy but discrete variable. But I wonder whether I should not change it and use this as dummy variable. There are only 6 entries that have 4+ garage bays, I thought it doesn't affect it too much.*

```{r}
#Pool:"Y" = 1; 0 otherwise
#la.house.data$pool
la.house.data$pool = ifelse(la.house.data$pool=='Y',1,0)  
#When I was doing the homework I checked to see that the command was successful via 'la.house.data$pool'
```

*Because I think the type of housing makes a huge difference in the price (perhaps an interaction variable somewhere?), I definitely need to deal with the 39 unclasssified values for the categorical variable 'type'.*

```{r}
tapply(la.house.data$price, la.house.data$type, mean)
la.house.data$type = ifelse(la.house.data$type=='', 'Alternative', as.character(la.house.data$type))
tapply(la.house.data$price, la.house.data$type, mean)
```

*Both price and square footage have a significant right skew; I needed to take a natural log of each so that the variables are less skewed.*

```{r}
par(mfrow=c(2,2))
hist(la.house.data$price, breaks=100000)
hist(la.house.data$sqft, breaks=1000)
hist(log(la.house.data$price),breaks=100)
hist(log(la.house.data$sqft), breaks=100)

#almost everything model with intercation between square footage and type
summary(la.model1 <- lm(log(price) ~ log(sqft)*type + bed + bath + pool + garage, la.house.data))

#removed interaction between square footage and type
summary(la.model2 <- lm(log(price) ~ log(sqft) + type + bed + bath + pool + garage, la.house.data))

#removed type because not as significant relative to the other variables
summary(la.model3 <- lm(log(price) ~ log(sqft) + bed + bath + pool + garage, la.house.data))

library(stargazer); library(knitr)

stargazer(la.model1,la.model2,la.model3,type = 'text', omit.stat = c("f", "rsq"), column.labels = c('Model 1', 'Model 2', 'Model 3'), model.names = F, model.numbers = F)
```

_Model 1 regresses the log of price on the interaction term log of square footage*(type) (alternative, townhome/condo, sing family residence), bedrooms, bathrooms, if a house has a pool, and the number of garage bays._ 

*Model 2 removed the interaction between square footage and type. In Model 3, I removed type because it did not appear to be as significant a contributor to the price relative to the other variables.*

*I need to test for goodness of fit...*
```{r}
anova(la.model1, la.model2, la.model3)
BIC(la.model1, la.model2, la.model3)
```
*ANOVA indicates that Model 3 is the "best" model. However, the BIC scores for the models that included 'type' appear to contradict my intrpetation that the interaction of type with square foootage (Model 1) or the addition of type (Model 2) did not add significantly to the predictive ability of the model.*

(@) Demonstrate the goodness-of-fit of your model (i.e, show that key assumptions appear to be met and that the model would seem to be a viable basis for inference). 

__Key Asssumptions__  
1. Linear Relationship  
2. Residuals are 'iid'; identically & independently distributed (errors $\epsilon_i$ do not affect each other).  
3. Nothing about $\beta_1$ affects $\beta_2$, ..., $\beta_k$.  


######Condition 1: Linearity
```{r}
par(mfrow = c(1,1)) # This sets the plot display back to one graph at a time.
plot(rstandard(la.model3))
abline(h = 0, lty = 1, col = "firebrick1")
abline(h= 2,lty=2, col = "orangered")
abline(h=-2,lty=2, col = "orangered")
```

######Condition 2: Nearly Normal Residuals
```{r}
hist(la.model3$residuals, breaks = 100)
#Q-Q Plot
plot(la.model3,2)
#Residual v. Fitted Plot
plot(la.model3,1)
```

######Condition 3: Constant Variability
```{r}
#Residuals v. Leverage Plot w/Cook's Distance
plot(la.model3,5)
```

(@) Interpret your substantive findings.
```{r}
summary(la.model3)
```

*By taking the natural log of price, we assume that the dependent variable is linear in percentage terms. This means that the linear coefficients are most easily be interpreted by exponentiating the given coefficient to produce a multiplicative effect.* 
*For instance, the equation would look like this:*

$\log(price) = \beta_{0} + \beta_{1}*lgsqrt + \beta_{2}*[(-0.106)*bed] + \beta_{3}*[(0.047)*bath] + \beta_{4}*[(0.281)*pool] + \beta_{5}*[-1.146)*garage]$

*The price of the house is affected by the following variables (holding all others constant):*

1. Each additional 1.40% increase in square footage can increase the price by 1.445139 or 1.45%.  
2. Each additional bedroom reduces the price for the same square footage by 11.9% (-0.11950).  
3. Each additional bathroom increases the price for the same square footage by 5.29% (0.05285).  
4. A pool can increase the prices of a house of equivalent square footage by 27.4% (0.27418).  
5. Each additional garage bay reduces the price of the house with equivalent square footage by apprximately 32 or 33% for a one or two car garage, although there are a range of values from the least 19.4% (garage3 -0.19386) to a maximum of 49.3% (garage4 -0.49312).  

(@) Discuss any potential shortcomings of this model and key future directions that you might take if you sought to better understand LA housing prices. 

*The most obvious shortcoming is that the data does not provide any geo-spatial information; it doesn't include a variable that can help to provide information by neighborhoods. Everyone knows that "location, location, location" drives housing prices, but there are different neighborhood location specfic attributes (quality of public schools, proximity to work/shopping, safety, walkability) that can contribute to or detract from from desireability expressed by the prices of the available housing stock within neighborhoods.* 

*The data need to allow researchers to "drill down" to a finer grain so that we can better see the contributors to variations in housing prices by having a better reference point; comparable housing within a specific neighborhood area.*

### Problem 4.

(@) Again, using the LA housing price data, __fit a model that estimates sqrt(price) solely as a function of sqrt(square footage) using maximum likelihood estimation (MLE)__ (hint: you'll need to use the `mle` function from the `stats4` package).   
Recall that in a linear regression, we assume that the residuals are normally distributed, so for MLE in this case we want a likelihood function that fits a normal distribution to the residuals:

```{r eval=FALSE}
library(stats4)
#Note: you'll need to edit this slightly to make it work for your data
LL <- function(beta0, beta1, mu, sigma) {
    R = y - x * beta1 - beta0
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
```
######Maximum Likelihood Estimation Approach
```{r}
LL <- function(beta0, beta1, mu, sigma) {
    R = sqrt(la.house.data$price) - sqrt(la.house.data$sqft) * beta1 - beta0
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
library(stats4)
m.l.e. = mle(LL, start = list(beta0 = 10, beta1 = 10, sigma = 1), fixed = list(mu = 0), nobs = length(la.house.data$price))

summary(m.l.e.)
```

(@) Try several different starting parameter values: How consistent are your results? Do your results typically match up with results from a simple linear regression? What do you think accounts for your results?

######OLS Approach
```{r}
summary(lm(sqrt(price) ~ sqrt(sqft), data = la.house.data))
```

*A one-unit increase in the square root of SqFt is predicted to increase the square root of the house price by 37.78, while for the OLS model the same coefficient is 37.91.* 
*The primary reason for the difference is that the MLE approach models the variance and provides an estimate for sigma; whereas the OLS approach does not.* 

*Also, the better the model fit the better you’ll be able to estimate things. For instance, if you don’t transform sqft and price, the model performs more poorly.*

(@) Perform the same analysis with but with the addition of a variable for number of bathrooms (i.e., $price = size + bathrooms$).
```{r}
LL2 <- function(beta0, beta1, beta2, mu, sigma) {
    R = sqrt(la.house.data$price) - sqrt(la.house.data$sqft) * beta1 - beta0 - la.house.data$bath * beta2 
    #
    R = suppressWarnings(dnorm(R, mu, sigma, log = TRUE))
    #
    -sum(R)
}
library(stats4)
m.l.e.2 = mle(LL2, start = list(beta0 = 10, beta1 = 10, beta2 = 10,sigma=1),
             fixed = list(mu = 0),
             nobs = length(la.house.data$price))

summary(m.l.e.2)
```
(@) In NO MORE than 4-6 sentences, explain how maximum likelihood estimation works in the context of this problem (i.e, how might you briefly describe your modeling approach within the context of a journal article methods section?)

*The general tactic in maximum likelihood estimation is to estimate a value for an unknown parameter (e.g., θ) that maximizes the probability of observing that data that are observed.*   
*For the model above, this means that we identify estimates for* $\beta_0$, $\beta_1$, and $\beta_3$ *that maximize the likelihood function of observing the data we observe within the data set assuming that the error terms $\epsilon_i$ are normally distributed with mean 0.*  
*In short, MLE maximizes the "agreement" of the selected model with the observed data.*

### Report your process

You're encouraged to reflect on what was hard/easy, problems you solved, helpful tutorials you read, etc. Give credit to your sources, whether it's a blog post, a fellow student, an online tutorial, etc.

### Rubric

Minus: Didn't tackle at least 3 tasks. Or didn't make companion graphs. Didn't interpret anything but left it all to the "reader". Or more than one technical problem that is relatively easy to fix. It's hard to find the report in our repo.

Check: Completed, but not fully accurate and/or readable. Requires a bit of detective work on my part to see what you did

Check plus: Hits all the elements. No obvious mistakes. Pleasant to read. No heroic detective work required. Solid.



#### The command below is helpful for debugging, please don't change it

```{r echo=FALSE}
sessionInfo()
```









